## Practical Black-Box Attacks against Machine Learning

已有的对抗式样本攻击需要：模型内部的信息或者训练数据

* 细节的DNN结构信息和参数信息 -- 替代模型
* 独立收集的大量的同一个分布采样的训练集去fit一个辅助模型 -- 人造数据集由oracle标注

threat model: 无法获得模型内部信息也无法获得训练数据，仅仅可以获得给一个数据，返回的oracle的label

### 本文的黑盒攻击过程：

初始化一些训练数据, label由访问oracle给出， 用这些数据训练我们自己的替代模型，训练好了以后使用基于雅可比方法的数据集扩充，然后在新的数据集中继续训练。迭代多轮候替代模型训练好，然后使用白盒攻击的方法攻击这个替代模型。替代模型和oracle模型有着相似的决策边界。

基于雅可比方法的数据集扩充：并不是为了提高替代模型的准确性，而是为了近似oracle的决策边界

### 黑盒攻击特点：

* 需要的能力仅仅局限于观察到oracle输出的label
* query oracle的次数是受限制的
* 这个方法可以泛化到其他分类模型

替代模型的结构：给一个大方向即可比如CNN，具体的几层，layersize不太重要。

生成人造数据：

* 首先不可能生成所有数据 - 会被检测，开销大
* 添加高斯噪声随机选择额外的点？不能学习，因为噪声不能代表输入的分布。

heuristic:

 识别模型输出改变的方向，在初始训练点的附近。这个方向直觉上需要更多的输入输出对去捕捉Oracle 的输出的变化。heuristic会优先这些采样，为了近似oracle的决策边界。方向通过substitute DNN的雅可比矩阵确定。添加的点能够更好的模仿决策边界s

why work?





贡献：

* MetaMind DNN分类器 84.24%错误率
* 黑盒攻击被校准：1. 减少query数量 2. 最大化分类错误率
* 其他ML模型比如线性回归，Amazon Google错误率分别 96.19% 88.94%
* 黑盒攻击可以避免防御机制

----



## GenAttack: Practical Black-box Attacks with Gradient-Free Optimization

现有的黑盒方法需要大量的query, 要么训练替代网络，要么从output scores上估计梯度。

本文使用遗传算法生成黑盒设定下的对抗样本。本文可以数量级上减少query数量和现有方法比。可以attack的方法

* state-of-the-art ImageNet defense
* ensemble adversarial training
* non-differentiable, randomized input transformation defenses

against ensemble adversarial training 表明可以高效的利用防御者的弱点进行直接黑盒攻击。

against non-differentiable, randomized input transformation defenses gradient-free 的特点表明可以绕过gradient masking/obfuscation

population-based 优化是一个有前途的研究方向into gradient-free black box attacks.



上一篇文章不好的地方：不完美的transferability 以及 computation cost  of 训练替代网络。

坐标有限差分方法：直接从confidence scores 估计梯度，然而计算上依然开销很大，并且依赖于优化的trick。

两种方法都query intensive

### Motivation

以往的方法都只考虑基于梯度的优化方法，然而用于攻击的目标函数都是有多个目标的向量值，比如目标错误分类攻击并且最小化output distortion。可取解的总的排序不可能。启发是每轮找一个解很容易找到局部最优，不能高效探索状态空间。

### GenAttack

powerful and efficient. 采取了population-based 方法使用遗传算法，更擅长多目标优化问题，更有弹性的对于不好的局部最优 -- 减少了query次数。因为不使用梯度，更加robust对于gradient masking and gradient obfuscation

优势

擅长targeted black-box attacks, 更少的query次数和以前方法比。Genattack 在ensemble adversarial attack the stateof-the-art ImageNet defense, and randomized, non-differentiable input transformation defenses上很成功



白盒攻击：

FGSM Explaining and harnessing adversarial examples. https://arxiv.org/pdf/1412.6572.pdf

![1541684098130](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1541684098130.png)

PGD: 

![1541684238837](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1541684238837.png)

C & W & EAD



黑盒：

Substitute Network

Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models

防御机制

Adversarial Training

Input  Transformation



## DECISION-BASED ADVERSARIAL ATTACKS: RELIABLE ATTACKS AGAINST BLACK-BOX MACHIN LEARNING MODELS
简单，几乎没有超参数调试，不依赖于替代模型，和基于梯度的方法比有同等的竞争性。

### Contribution



这个下单的动作应该是所有这类人的下单量的累计和吧。

1. 每单固定量的话

